{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID2214 Assignment 3 Group no. [7]\n",
    "### Project members: \n",
    "[Cecilia Battinelly, cbat@kth.se]\n",
    "[Valgerdur Tryggvadottir, vtry@kth.se]\n",
    "[Parastu Rahgozar, parastu@kth.se]\n",
    "[Zhiwu Dong, zhiwu@kth.se\n",
    "\n",
    "### Declaration\n",
    "By submitting this solution, it is hereby declared that all individuals listed above have contributed to the solution, either with code that appear in the final solution below, or with code that has been evaluated and compared to the final solution, but for some reason has been excluded. It is also declared that all project members fully understand all parts of the final solution and can explain it upon request.\n",
    "\n",
    "It is furthermore declared that the code below is a contribution by the project members only, and specifically that no part of the solution has been copied from any other source (except for lecture slides at the course ID2214) and no part of the solution has been provided by someone not listed as project member above.\n",
    "\n",
    "It is furthermore declared that it has been understood that no other library/package than the Python 3 standard library, NumPy, pandas and time may be used in the solution for this assignment.\n",
    "\n",
    "### Instructions\n",
    "All assignments starting with number 1 below are mandatory. Satisfactory solutions\n",
    "will give 1 point (in total). If they in addition are good (all parts work more or less \n",
    "as they should), completed on time (submitted before the deadline in Canvas) and according\n",
    "to the instructions, together with satisfactory solutions of assignments starting with \n",
    "number 2 below, then the assignment will receive 2 points (in total).\n",
    "\n",
    "It is highly recommended that you do not develop the code directly within the notebook\n",
    "but that you copy the comments and test cases to your regular development environment\n",
    "and only when everything works as expected, that you paste your functions into this\n",
    "notebook, do a final testing (all cells should succeed) and submit the whole notebook \n",
    "(a single file) in Canvas (do not forget to fill in your group number and names above).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NumPy, pandas and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reused functions from Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste functions from Assignment 1 here that you need for this assignment\n",
    "\n",
    "# CREATE BINS FUNCTION\n",
    "def create_bins(dff,nobins=10,bintype=\"equal-width\"):\n",
    "    d = {}\n",
    "    df = dff.copy()\n",
    "    if bintype==\"equal-width\":\n",
    "        for elem in df.columns:\n",
    "            if elem!='ID' and elem!='CLASS':\n",
    "                values = df[elem].values\n",
    "                res, bins = pd.cut(values,nobins,retbins=True,labels=False)\n",
    "                bins=list(bins)\n",
    "                bins[0] = -np.inf\n",
    "                bins[-1] = np.inf\n",
    "                d[elem] = bins #mapping\n",
    "                df[elem]= res\n",
    "                df[elem] = df[elem].astype(\"category\")\n",
    "                df[elem].cat.categories\n",
    "                lab = list(range(0,nobins))\n",
    "                df[elem] = df[elem].cat.set_categories(lab)\n",
    "    else:\n",
    "        for elem in df.columns:\n",
    "            if elem!='ID' and elem!='CLASS':\n",
    "                values = df[elem].values\n",
    "                res, bins = pd.qcut(values,nobins,retbins=True,labels=False, duplicates='drop')\n",
    "                bins = list(bins)\n",
    "                bins[0] = -np.inf\n",
    "                bins[-1]= np.inf\n",
    "                d[elem] =  bins #mapping\n",
    "                df[elem] = res\n",
    "                df[elem] = df[elem].astype(\"category\")\n",
    "                df[elem].cat.categories\n",
    "                lab = list(range(0,nobins))\n",
    "                df[elem] = df[elem].cat.set_categories(lab)\n",
    "\n",
    "\n",
    "    return df, d\n",
    "\n",
    "\n",
    "# CREATE IMPUTATION FUNCTION\n",
    "def create_imputation(df1):\n",
    "    df = df1.copy()\n",
    "    d = {}\n",
    "    # Go through all the columns whose name is different from ID & CLASS\n",
    "    # First we analyze numerical columns and if all the values are missing\n",
    "    # we fill them with 0, otherwise we fill the missing values with the mean.\n",
    "    # We append to the dictionary the mean value of the column referring to the column\n",
    "    # name as key.\n",
    "\n",
    "    # Later(else) we go through object and category columns. If all values are missing\n",
    "    # we fill them with \"\" for object column, or first element for category column\n",
    "    # otherwise we fill with the first element of mode array.\n",
    "    # We append to the dictionary the mode value [0] of the column referring to the column\n",
    "    # name as key.\n",
    "    \n",
    "    for elem in df.columns:\n",
    "        if elem!='ID' and elem!='CLASS':\n",
    "            if df[elem].dtype == \"float64\" or df[elem].dtype == \"int64\":\n",
    "                values = df[elem].values     \n",
    "                if np.all(np.isnan(values)):\n",
    "                    df[elem].fillna(0, inplace = True)\n",
    "                else:\n",
    "                      df[elem].fillna(df[elem].mean(), inplace = True)\n",
    "                    \n",
    "\n",
    "                value = df[elem].mean() \n",
    "                d[elem] = value\n",
    "            else:\n",
    "                values = df[elem].values\n",
    "                df[elem].fillna(df[elem].mode()[0], inplace = True)\n",
    "\n",
    "                value2 = df[elem].mode()[0]\n",
    "                d[elem] = value2\n",
    "\n",
    "    return df, d\n",
    "\n",
    "\n",
    "# APPLY IMPUTATION\n",
    "def apply_imputation(df1,imputation):\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Go through all columns whose name is not ID or CLASS and fill the missing\n",
    "    # values reffering to the imputation dictionary, i.e, given the column name, we fill\n",
    "    # the missing value using the valueZ in dictionary (elem: valueZ).\n",
    "\n",
    "    for elem in df1.columns:\n",
    "        if elem!='ID' and elem!='CLASS':\n",
    "            tup_elem = imputation[elem]\n",
    "            df[elem].fillna(tup_elem, inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# APPLY BINS FUNCTION\n",
    "def apply_bins(dff,binning):\n",
    "    df = dff.copy()\n",
    "    for elem in df.columns:\n",
    "        if elem!='ID' and elem!='CLASS':\n",
    "                values = df[elem].values\n",
    "                bins = binning[elem]\n",
    "                res = pd.cut(values,bins,labels=False)\n",
    "                df[elem] = res\n",
    "                df[elem] = df[elem].astype(\"category\")\n",
    "                df[elem].cat.categories\n",
    "    return df\n",
    "\n",
    "\n",
    "# ACCURACY FUNCTION\n",
    "def accuracy(df, correctlabels):\n",
    "    dff = df.copy()\n",
    "    if len(correctlabels)!= len(dff.index):\n",
    "         print(\"Error, mismatching dimensions!\")\n",
    "         return None\n",
    "    maximum = dff.idxmax(axis=1)\n",
    "    num = 0\n",
    "    \n",
    "    for i in range(len(maximum)):\n",
    "        if maximum[i] == correctlabels[i]:\n",
    "            num += 1\n",
    "    den = len(maximum)\n",
    "    \n",
    "    frac = num / den\n",
    "\n",
    "    return frac\n",
    "\n",
    "\n",
    "# BRIER SCORE FUNCTION\n",
    "def brier_score(df,correctlabels):\n",
    "    # Create an empty list where all partial sums will be stored\n",
    "    score = []\n",
    "    \n",
    "    for i in df.index:\n",
    "        # Create partial sum for each row\n",
    "        summ = 0\n",
    "        row = np.array(df.loc[i])\n",
    "        \n",
    "        o = np.zeros(len(row))\n",
    "        indx = np.where(df.columns==correctlabels[i])[0]\n",
    "        o[indx] = 1\n",
    "        for i in range(len(row)):\n",
    "            summ += (row[i] - o[i])**2\n",
    "        score.append(summ)\n",
    "\n",
    "    brier = sum(score)/len(df.index)\n",
    "    \n",
    "    return brier\n",
    "\n",
    "\n",
    "# AUC FUNCTION\n",
    "def auc(df,correctlabels):\n",
    "    #Create a vector where to store partial AUCs for each class (i.e. column) \n",
    "    AUCT = [] \n",
    "    for col in df.columns:\n",
    "         # For each class create a dictionary to store {score: tp, fp}\n",
    "        d = {}\n",
    "        values = df[col].values\n",
    "        # Create array of scores values in decreasing order\n",
    "        val = sorted(values, reverse = True)  \n",
    "        # For each element in val iterate and set the threshold equal to the \n",
    "        # current value and label the instances accordingly to this \n",
    "        for elem in val:\n",
    "            # Current score = threshold = each value in the column iteratively\n",
    "            s = elem \n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            # For each score in the unordered list of scores check if the label is correct\n",
    "            # If so, increase by 1 tp; o/w increase by 1 fp\n",
    "            for j in range(len(values)):\n",
    "                if values[j] == s:\n",
    "                    label = col\n",
    "                    if label == correctlabels[j]:\n",
    "                        tp += 1\n",
    "                    else:\n",
    "                        fp += 1\n",
    "            # Append the values of fp, tp to the corresponding score\n",
    "            d[s] = [tp,fp]\n",
    "        # Implementation of AUC area algorithm for each class\n",
    "        Tot_tp = 0\n",
    "        Tot_fp = 0\n",
    "        for elem in d:\n",
    "            Tot_tp += d[elem][0]\n",
    "            Tot_fp += d[elem][1]\n",
    "        AUC = 0\n",
    "        Cov_tp = 0\n",
    "        for elem in d:\n",
    "            fp = d[elem][1]\n",
    "            tp = d[elem][0]\n",
    "            if fp == 0:\n",
    "                Cov_tp += tp\n",
    "            elif tp == 0:\n",
    "                AUC += (Cov_tp/Tot_tp)*(fp/Tot_fp)\n",
    "            else:\n",
    "                AUC += (Cov_tp/Tot_tp)*(fp/Tot_fp)+(tp/Tot_tp)*(fp/Tot_fp)/2\n",
    "                Cov_tp += tp\n",
    "        AUCT.append(AUC)\n",
    "    \n",
    "    fr = []\n",
    "    for col in df.columns:\n",
    "        freq = 0\n",
    "        for val in correctlabels:\n",
    "            if val == col:\n",
    "                freq += 1\n",
    "        freq = freq /len(correctlabels)\n",
    "        fr.append(freq)\n",
    "    \n",
    "    # Return weighted auc = return scalar product AUCT & frequency vector\n",
    "    return np.dot(fr,AUCT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the class DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class DecisionTree with three functions __init__, fit and predict (after the comments):\n",
    "#\n",
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# binning, imputatiom, labels, model\n",
    "#\n",
    "# Input to fit:\n",
    "# self: the object itself\n",
    "# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n",
    "# nobins: no. of bins (default = 10)\n",
    "# bintype: either \"equal-width\" (default) or \"equal-size\"\n",
    "# min_samples_split: no. of instances required to allow a split (default = 5)\n",
    "#\n",
    "# Output from fit:\n",
    "# nothing\n",
    "#\n",
    "# The result of applying this function should be:\n",
    "#\n",
    "# self.binning should be a discretization mapping (see Assignment 1) from df\n",
    "# self.imputation should be an imputation mapping (see Assignment 1) from df\n",
    "# self.labels should be the categories of the \"CLASS\" column of df, set to be of type \"category\" \n",
    "# self.model should be a decision tree (for details, see lecture slides), where the leafs return class probabilities\n",
    "# Note that the function does not return anything but just assigns values to the attributes of the object.\n",
    "#\n",
    "# Hint 1: First find the available features (excluding \"CLASS\" and \"ID\"), then find the class counts, e.g., using \n",
    "#         groupby, and calculate the default class probabilities (relative frequencies of the class labels)\n",
    "# Hint 2: Define a function, e.g., called divide_and_conquer, that takes the above as input together with df \n",
    "#         and min_samples_split, and also a nodeno (starting with 0) to keep track of the generated nodes in the tree\n",
    "# Hint 3: You may represent the tree under construction as a list of nodes (tuples), on the form:\n",
    "#         (nodeno,\"leaf\",class_probabilities): corresponding to a leaf node where class_probabilities is a vector\n",
    "#                                              with the relative class frequencies (ordered according to self.labels)\n",
    "#         (nodeno,feature,node_dict): corresponding to an internal (non-leaf) node where node_dict is a mapping from\n",
    "#                                     the possible values of feature to child nodes (their nodenos)\n",
    "# Hint 4: You may evaluate each feature by a function information_content, which takes the group sizes\n",
    "#         for each possible value of the feature together with the class counts of each group as input\n",
    "# Hint 5: The best feature found (with lowest resulting information content) will be used to split the training\n",
    "#         instances, and each sub-group is used for generating a sub-tree (recursively by divide_and_conquer,\n",
    "#         see lecture slides for details)\n",
    "# Hint 6: The list of nodes output by divide_and_conquer may finally be converted to an array, where each nodeno in the \n",
    "#         tuples corresponds to an index of the array \n",
    "#\n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "#              predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "#              are the relative class frequencies in the leaves of the decision tree into which the instances in\n",
    "#              df fall\n",
    "#\n",
    "# Hint 1: Drop any \"CLASS\" and \"ID\" columns first and then apply imputation and binning\n",
    "# Hint 2: Iterate over the rows calling some sub-function, e.g., make_prediction(nodeno,row), which for a test row\n",
    "#         finds a leaf node from which class probabilities are obtained\n",
    "# Hint 3: This sub-function may recursively traverse the tree (represented by an array), starting with the nodeno\n",
    "#         that corresponds to the root\n",
    "\n",
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.binning = None\n",
    "        self.imputation = None\n",
    "        self.labels = None\n",
    "        self.model = None\n",
    "\n",
    "\n",
    "        \n",
    "    def fit(self, df, nobins=10, bintype=\"equal-width\", min_samples_split = 5):\n",
    "        \n",
    "        # self.imputation should be an imputation mapping from df\n",
    "        df1, imp_map = create_imputation(df) \n",
    "        self.imputation = imp_map\n",
    "        \n",
    "        # self.binning should be a discretization mapping from df\n",
    "        df2, disc_map = create_bins(df1, nobins, bintype)\n",
    "        self.binning = disc_map\n",
    "        \n",
    "        # self.labels should be the categories of the \"CLASS\" column of df, set to be of type \"category\" \n",
    "        classes = df[\"CLASS\"].astype(\"category\")  # gives the column \"CLASS\", each column in DataFrame is of type Series\n",
    "        self.labels = list(set(classes))      \n",
    "        \n",
    "\n",
    "        # Hint 1: First find the available features (excluding \"CLASS\" and \"ID\"), then find the class counts, e.g., using \n",
    "        #         groupby, and calculate the default class probabilities (relative frequencies of the class labels)\n",
    "        \n",
    "        # Available features = coulmn names except \"CLASS\" and \"ID\"\n",
    "        df_features = df2.drop([\"ID\", \"CLASS\"], axis = 1)  \n",
    "        features = list(df_features.columns.values)\n",
    "        \n",
    "        # relative frequencies  \n",
    "        classprior = dict(df2.groupby(\"CLASS\").size()/len(df2))    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Hint: Define a function, e.g., called divide_and_conquer, that takes the above as input together with df \n",
    "        #         and min_samples_split, and also a nodeno (starting with 0) to keep track of the generated nodes in the tree\n",
    "        def divide_and_conquer(df, feature, nodeno, min_samples_split, class_prob=classprior):\n",
    "    \n",
    "                T = []\n",
    "                \n",
    "                # ----- STEP 1 -----:  There are three stopping criterias, we start by checking them    \n",
    "\n",
    "                #If #instances is lower than the minimum number of instances needed to do a split => return the default probabilities\n",
    "                if len(df) < min_samples_split: \n",
    "                    new_class_prob = dict(df.groupby(\"CLASS\").size()/len(df))\n",
    "                    node = (nodeno,\"leaf\", class_prob) \n",
    "                    T.append(node) # Add the tuple to the final tree & return it\n",
    "                    return T\n",
    "                \n",
    "                #If there is only 1 class, i.e. all instances have same class label\n",
    "                elif len(list(set(df[\"CLASS\"])))==1: \n",
    "                    new_class_prob = dict(df.groupby(\"CLASS\").size()/len(df)) #always 1\n",
    "                    node = (nodeno, \"leaf\", new_class_prob)\n",
    "                    T.append(node)\n",
    "                    return T\n",
    "                \n",
    "                # If there are no more features to use to do split\n",
    "                elif len(feature) == 0:\n",
    "                    new_class_prob = dict(df.groupby(\"CLASS\").size()/len(df))\n",
    "                    node = (nodeno,\"leaf\", new_class_prob)\n",
    "                    T.append(node)\n",
    "                    return T\n",
    "                \n",
    "                \n",
    "                \n",
    "                # Else, we are not in a leaf and we should call the function again\n",
    "                else:\n",
    "                    # ----- STEP 2 -----:  Define the information_content function\n",
    "                    # Hint: You may evaluate EACH feature by a function information_content, which takes the group sizes\n",
    "                    #       for each possible value of the feature together with the class counts of each group as input\n",
    "                    def information_content(group_size, class_count_group):\n",
    "                                            \n",
    "                        # Want the information content to be as low as possible (so this is just the second term\n",
    "                        # of the information gain fromula)\n",
    "                        # Information content = sum over all groups of feature[(group_size/total)*entropy(group)]\n",
    "                        # Entropy(group) = sum over possible classes of that group (-pi log (pi))  => use class_count_group\n",
    "                        \n",
    "                        IC = 0  #information content initialized\n",
    "                        total = sum(group_size)\n",
    "                        for group in dict(group_size).keys():  # the groups are the values of bins\n",
    "                            entropy = 0 #for each group, so have to initialize here as 0 \n",
    "                            if group_size[group] != 0:  #because if group size is zero it will not appear in class_count_group\n",
    "                                for amount in class_count_group[group]:\n",
    "                                    p = amount/group_size[group]\n",
    "                                    entropy -= p*np.log2(p)\n",
    "    \n",
    "                                IC += (group_size[group]/total)*entropy\n",
    "                        \n",
    "                        \n",
    "                        return IC  \n",
    "                \n",
    "                \n",
    "                    # ----- STEP 3 -----:  find where to split\n",
    "                    # Now we can use the information_content function\n",
    "                    group_size = 0 \n",
    "                    IC_list = []\n",
    "                    \n",
    "                    # Call the function for each feature\n",
    "                    for i in feature:  \n",
    "                        group_size = df.groupby(i).size()\n",
    "                        class_count_group = df.groupby([i, \"CLASS\"]).size() \n",
    "                        IC = information_content(group_size, class_count_group)\n",
    "                        IC_list.append(IC)\n",
    "                   \n",
    "                    # Hint: The best feature found (with lowest resulting information content) will be used to split the training\n",
    "                    #         instances, and each sub-group is used for generating a sub-tree (recursively by divide_and_conquer,\n",
    "                    #         see lecture slides for details)\n",
    "                    min_index = np.argmin(IC_list)  # This is the index of the feature we want to split\n",
    "                    f = feature[min_index] #the feature where to split\n",
    "                    feature.remove(f) #remove the splitting feature (because we are only allowed to use each one once to split)\n",
    "                    \n",
    "                    \n",
    "                    # ----- STEP 4 -----:  Split and call the the function recursively\n",
    "    \n",
    "                    # Create a dictionary to store child node for each value of the column f\n",
    "                    nodedict = {} \n",
    "                    classCountI = dict(df.groupby(\"CLASS\").size()/len(df)) #Current default priors for class\n",
    "                    for val, df_val in df.groupby(f):\n",
    "                        newnodeno = nodeno + 1 # Here we increase the node on a level of depth (so nodeno is the depth of the tree)\n",
    "                        T_val = divide_and_conquer(df_val, feature, newnodeno, min_samples_split, class_prob = classCountI)\n",
    "                        nodedict[val] = T_val # Will create a nested dictionary\n",
    "                \n",
    "                    # define nodes and append to T\n",
    "                    node = (nodeno, f ,nodedict)  \n",
    "                    T.append(node)\n",
    "                    \n",
    "                    return T\n",
    "\n",
    "        \n",
    "        # nodeno should first be 0 \n",
    "        nodeno = 0\n",
    "        self.model = divide_and_conquer(df2, features, nodeno, min_samples_split, classprior)\n",
    "        \n",
    "        \n",
    "       \n",
    "    \n",
    "    def predict(self, df):\n",
    "        \n",
    "        # Hint 1: Drop any \"CLASS\" and \"ID\" columns first and then apply imputation and binning\n",
    "        df_copy = df.drop([\"CLASS\", \"ID\"], axis = 1)\n",
    "        df_imputation = apply_imputation(df_copy, self.imputation)\n",
    "        df_binning = apply_bins(df_imputation, self.binning)\n",
    "        \n",
    "        # Get the list of features names for making predictions \n",
    "        features = list(df_binning.columns.values)\n",
    "        \n",
    "        \n",
    "        # The following func makes a prediction for an instance (i.e. a row of the test df)\n",
    "        # using the tree, i.e. self.model and the features to easily access the tree\n",
    "        def make_prediction(tree, row, features):\n",
    "            \n",
    "            col = tree[0][1] # Splitting feature \n",
    "            feat = np.array(features) \n",
    "            col_num = np.where(feat==col) # Get the index of the splitting feature to access\n",
    "            value = row[col_num]  # its value in the row\n",
    "            \n",
    "            # Now we want to find this values among the childnodes of the splitting feature\n",
    "            subtree = list(tree[0][2].items()) \n",
    "            # We access to subtree, i.e. node_dict which is storing for each value of splitting feature its subsubtrees\n",
    "            for key, elem in subtree:\n",
    "                if key == value and \"leaf\" in elem[0][1]:\n",
    "                    # This means we are ended up in a leaf and we convert the \n",
    "                    # dictionary of prob into vector\n",
    "                    pr = np.zeros(len(self.labels))\n",
    "                    classprob = elem[0][2].items()\n",
    "                    labels = np.array(self.labels)\n",
    "                    \n",
    "                    # Conversion from dictionary into array for probabilities\n",
    "                    for clas,prob in classprob:\n",
    "                         idx = np.where(labels==clas)\n",
    "                         pr[idx] = prob\n",
    "\n",
    "                    return pr\n",
    "\n",
    "                elif key == value and \"leaf\" not in elem[0][1]:\n",
    "                    # This means that the value does not correspond to a lef\n",
    "                    # We need to go further down the tree to investigate and we call the func\n",
    "                    # again on the subtree\n",
    "                    return make_prediction(elem, row, features)\n",
    "\n",
    "                    \n",
    "                \n",
    "        prediction_list = []\n",
    "        for i in range(len(df_binning.values)):\n",
    "            row = df_binning.values[i]\n",
    "            prob = make_prediction(self.model, row, features) #call the function make_prediction\n",
    "            prediction_list.append(prob)\n",
    "        \n",
    "        predictions = pd.DataFrame(prediction_list, columns = self.labels)  \n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (5, 'equal-width', 3): 0.26 s.\n",
      "Testing time (5, 'equal-width', 3): 0.12 s.\n",
      "Training time (5, 'equal-width', 5): 0.26 s.\n",
      "Testing time (5, 'equal-width', 5): 0.14 s.\n",
      "Training time (5, 'equal-width', 10): 0.20 s.\n",
      "Testing time (5, 'equal-width', 10): 0.12 s.\n",
      "Training time (5, 'equal-size', 3): 0.21 s.\n",
      "Testing time (5, 'equal-size', 3): 0.09 s.\n",
      "Training time (5, 'equal-size', 5): 0.22 s.\n",
      "Testing time (5, 'equal-size', 5): 0.10 s.\n",
      "Training time (5, 'equal-size', 10): 0.22 s.\n",
      "Testing time (5, 'equal-size', 10): 0.09 s.\n",
      "Training time (10, 'equal-width', 3): 0.29 s.\n",
      "Testing time (10, 'equal-width', 3): 0.08 s.\n",
      "Training time (10, 'equal-width', 5): 0.30 s.\n",
      "Testing time (10, 'equal-width', 5): 0.08 s.\n",
      "Training time (10, 'equal-width', 10): 0.43 s.\n",
      "Testing time (10, 'equal-width', 10): 0.10 s.\n",
      "Training time (10, 'equal-size', 3): 0.30 s.\n",
      "Testing time (10, 'equal-size', 3): 0.10 s.\n",
      "Training time (10, 'equal-size', 5): 0.30 s.\n",
      "Testing time (10, 'equal-size', 5): 0.10 s.\n",
      "Training time (10, 'equal-size', 10): 0.31 s.\n",
      "Testing time (10, 'equal-size', 10): 0.10 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-width</th>\n",
       "      <th>3</th>\n",
       "      <td>0.485981</td>\n",
       "      <td>0.563166</td>\n",
       "      <td>0.668203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.429907</td>\n",
       "      <td>0.610763</td>\n",
       "      <td>0.646016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.570093</td>\n",
       "      <td>0.529065</td>\n",
       "      <td>0.815652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-size</th>\n",
       "      <th>3</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.664381</td>\n",
       "      <td>0.692070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.629125</td>\n",
       "      <td>0.749457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.638374</td>\n",
       "      <td>0.686873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">10</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-width</th>\n",
       "      <th>3</th>\n",
       "      <td>0.383178</td>\n",
       "      <td>0.618840</td>\n",
       "      <td>0.638728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.588785</td>\n",
       "      <td>0.500753</td>\n",
       "      <td>0.840560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.520606</td>\n",
       "      <td>0.820311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-size</th>\n",
       "      <th>3</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.694005</td>\n",
       "      <td>0.654757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.656426</td>\n",
       "      <td>0.672517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>0.659044</td>\n",
       "      <td>0.663924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Accuracy  Brier score       AUC\n",
       "5  equal-width 3   0.485981     0.563166  0.668203\n",
       "               5   0.429907     0.610763  0.646016\n",
       "               10  0.570093     0.529065  0.815652\n",
       "   equal-size  3   0.542056     0.664381  0.692070\n",
       "               5   0.542056     0.629125  0.749457\n",
       "               10  0.542056     0.638374  0.686873\n",
       "10 equal-width 3   0.383178     0.618840  0.638728\n",
       "               5   0.588785     0.500753  0.840560\n",
       "               10  0.607477     0.520606  0.820311\n",
       "   equal-size  3   0.542056     0.694005  0.654757\n",
       "               5   0.542056     0.656426  0.672517\n",
       "               10  0.542056     0.659044  0.663924"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
    "\n",
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "tree_model = DecisionTree()\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "nobins_values = [5,10]\n",
    "bintype_values = [\"equal-width\",\"equal-size\"]\n",
    "min_samples_split_values = [3,5,10]\n",
    "parameters = [(nobins,bintype,min_samples_split) for nobins in nobins_values for bintype in bintype_values \n",
    "              for min_samples_split in min_samples_split_values]\n",
    "\n",
    "results = np.empty((len(parameters),3))\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    t0 = time.perf_counter()\n",
    "    tree_model.fit(glass_train_df,nobins=parameters[i][0],bintype=parameters[i][1],min_samples_split=parameters[i][2])\n",
    "    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = tree_model.predict(glass_test_df)\n",
    "    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=pd.MultiIndex.from_product([nobins_values,bintype_values,min_samples_split_values]),\n",
    "                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.64\n",
      "AUC on training set: 0.85\n",
      "Brier score on training set: 0.41\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "tree_model.fit(glass_train_df,min_samples_split=1)\n",
    "predictions = tree_model.predict(glass_train_df)\n",
    "print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on assumptions, things that do not work properly, etc.\n",
    "The code is not returning the exactly correct values. In order to find the problem, the first thing that\n",
    "we checked was the correctness of variables: features and classprior. Later, we checked the\n",
    "correctness of the information_content function; we double checked calculating the information\n",
    "residual for Mg by hand and also double checked using the dataframe on slide 9 from Decision\n",
    "Tree lecture and it returned the correct values.\n",
    "\n",
    "Later on, as we thought to define the tree according to levels which is a very common\n",
    "representation - specifically, some of group members were taught so in other courses – we\n",
    "checked the correctness of the predict function, running the code on a smaller glass_test dataset.\n",
    "The function was able to access to all the nodes and move through the tree correctly. Finally, we\n",
    "checked the function divide_and_conquer against the slide 3 from Decision Tree lecture. The\n",
    "three stopping criteria (first three if conditions) are equivalent to the algorithm in slides and we also\n",
    "discussed with you their correctness. Later on, we checked how the sub-dataframe I_i for each\n",
    "value v_i for the splitting feature were generated and it worked as expected.\n",
    "\n",
    "When debugging, it was also tried to number the nodes uniquely using either a global nodeno\n",
    "variable or letting divide_and_conquer returning current_nodeno and this was very difficult for us to\n",
    "think as we were so confident with a level-depth representation.\n",
    "\n",
    "Moreover, as discussed with you on Wednesday 28 th of November, the “when observing\n",
    "categorical feature values in the test set that were not observed in the training set” problem was\n",
    "handled correctly in the create_bins function.\n",
    "\n",
    "Given all this, we think the mistake could be in a stopping criterion or eventually in the\n",
    "divide_and_conquer function. However, we think that the mistake is not connected to the tree\n",
    "representation but rather to a misunderstanding of the divide_and_conquer algorithm.\n",
    "Furthermore, we also think this error could be a minor error as the scores are all plausible values\n",
    "as well as the training-testing times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the class DecisionForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class DecisionForest with three functions __init__, fit and predict (after the comments):\n",
    "#\n",
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# binning, imputatiom, labels, model\n",
    "#\n",
    "# Input to fit:\n",
    "# self: the object itself\n",
    "# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n",
    "# nobins: no. of bins (default = 10)\n",
    "# bintype: either \"equal-width\" (default) or \"equal-size\"\n",
    "# min_samples_split: no. of instances required to allow a split (default = 5)\n",
    "# random_features: no. of features to evaluate at each split (default = 2), 0 means all features (no random sampling)\n",
    "# notrees: no. of trees in the forest (default = 10)\n",
    "#\n",
    "# Output from fit:\n",
    "# nothing\n",
    "#\n",
    "# The result of applying this function should be:\n",
    "#\n",
    "# self.binning should be a discretization mapping (see Assignment 1) from df\n",
    "# self.imputation should be an imputation mapping (see Assignment 1) from df\n",
    "# self.labels should be the categories of the \"CLASS\" column of df, set to be of type \"category\" \n",
    "# self.model should be a random forest (for details, see lecture slides)\n",
    "# Note that the function does not return anything but just assigns values to the attributes of the object.\n",
    "#\n",
    "# Hint 1: Redefine divide_and_conquer to take one additional argument; random_features, and instead of\n",
    "#         evaluating all features choose a random subset, e.g., by np.random.choice (without replacement)\n",
    "# Hint 2: Generate each tree in the forest from a bootstrap replicate of df, e.g., by np.random.choice \n",
    "#         (with replacement) from the index values of df.\n",
    "#\n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "#              predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "#              are the mean of all relative class frequencies in the leaves of the forest into which the instances in\n",
    "#              df fall\n",
    "#\n",
    "# Hint 1: Drop any \"CLASS\" and \"ID\" columns first and then apply imputation and binning\n",
    "# Hint 2: Iterate over the rows calling some sub-function, e.g., make_prediction(row), which for a test row\n",
    "#         finds all leaf nodes and calculates the average of their class probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (1, 1): 7.51 s.\n",
      "Testing time (1, 1): 0.09 s.\n",
      "Training time (1, 2): 10.65 s.\n",
      "Testing time (1, 2): 0.09 s.\n",
      "Training time (1, 5): 19.78 s.\n",
      "Testing time (1, 5): 0.09 s.\n",
      "Training time (2, 1): 7.97 s.\n",
      "Testing time (2, 1): 0.09 s.\n",
      "Training time (2, 2): 12.01 s.\n",
      "Testing time (2, 2): 0.10 s.\n",
      "Training time (2, 5): 19.42 s.\n",
      "Testing time (2, 5): 0.10 s.\n",
      "Training time (5, 1): 4.57 s.\n",
      "Testing time (5, 1): 0.10 s.\n",
      "Training time (5, 2): 7.31 s.\n",
      "Testing time (5, 2): 0.09 s.\n",
      "Training time (5, 5): 9.75 s.\n",
      "Testing time (5, 5): 0.09 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.477912</td>\n",
       "      <td>0.860736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.710280</td>\n",
       "      <td>0.411981</td>\n",
       "      <td>0.892842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.710280</td>\n",
       "      <td>0.421511</td>\n",
       "      <td>0.895716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>1</th>\n",
       "      <td>0.663551</td>\n",
       "      <td>0.452421</td>\n",
       "      <td>0.872447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.626168</td>\n",
       "      <td>0.411684</td>\n",
       "      <td>0.911137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.654206</td>\n",
       "      <td>0.435847</td>\n",
       "      <td>0.881621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>1</th>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.457859</td>\n",
       "      <td>0.865636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.719626</td>\n",
       "      <td>0.410616</td>\n",
       "      <td>0.903016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.883897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy  Brier score       AUC\n",
       "1 1  0.644860     0.477912  0.860736\n",
       "  2  0.710280     0.411981  0.892842\n",
       "  5  0.710280     0.421511  0.895716\n",
       "2 1  0.663551     0.452421  0.872447\n",
       "  2  0.626168     0.411684  0.911137\n",
       "  5  0.654206     0.435847  0.881621\n",
       "5 1  0.644860     0.457859  0.865636\n",
       "  2  0.719626     0.410616  0.903016\n",
       "  5  0.616822     0.455782  0.883897"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "forest_model = DecisionForest()\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "min_samples_split_values = [1,2,5]\n",
    "random_features_values = [1,2,5]\n",
    "\n",
    "parameters = [(min_samples_split,random_features) for min_samples_split in min_samples_split_values \n",
    "              for random_features in random_features_values]\n",
    "\n",
    "results = np.empty((len(parameters),3))\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    t0 = time.perf_counter()\n",
    "    forest_model.fit(glass_train_df,min_samples_split=parameters[i][0],random_features=parameters[i][1])\n",
    "    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = forest_model.predict(glass_test_df)\n",
    "    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=pd.MultiIndex.from_product([min_samples_split_values,random_features_values]),\n",
    "                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.96\n",
      "AUC on training set: 1.00\n",
      "Brier score on training set: 0.12\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "forest_model.fit(glass_train_df,min_samples_split=1)\n",
    "predictions = forest_model.predict(glass_train_df)\n",
    "print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on assumptions, things that do not work properly, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
